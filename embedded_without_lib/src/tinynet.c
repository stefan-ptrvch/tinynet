#include<math.h>

// Number of layers in network
#define NUM_LAYERS 3

// Define macros for activation functions
#define LINEAR_ACTIVATION 1
#define SIGMOID_ACTIVATION 2
#define SOFTMAX_ACTIVATION 3

// Layer sizes
const int num_neurons[NUM_LAYERS] = {4, 100, 3};

// Weights matrices
const float weight_matrix_0[400] = {
-0.051167, 0.088812, 0.208765, -0.086333, 0.065307, 0.164082, -0.000966,
-0.206491, -0.043593, 0.163635, -0.358131, -0.065095, -0.210191, 0.055292,
0.128265, 0.123249, 0.034794, 0.278207, -0.300124, -0.427311, 0.145161,
-0.220553, 0.237084, -0.029427, 0.145628, -0.202347, -0.215281, 0.012959,
-0.107346, -0.217799, 0.397301, 0.380640, -0.082466, 0.047904, 0.132918,
-0.089075, 0.034804, 0.033286, -0.354967, -0.253006, 0.151439, -0.221943,
-0.037872, -0.216779, -0.049090, 0.078483, -0.178610, -0.159444, -0.039857,
0.192111, -0.210567, -0.329223, 0.050940, 0.117183, -0.361341, -0.270873,
-0.085893, -0.118302, -0.076174, -0.138920, -0.157134, 0.091695, 0.244940,
0.318760, -0.160010, 0.073644, 0.304996, 0.319828, -0.094701, 0.131271,
-0.177046, -0.284503, 0.063072, 0.215228, 0.074102, 0.046688, 0.031036,
0.288897, 0.092675, -0.097343, 0.135093, 0.014229, 0.003659, 0.187695,
0.150821, -0.143272, -0.378497, -0.094377, -0.202519, 0.073103, 0.017459,
-0.104035, -0.051625, -0.136876, -0.141800, -0.074134, 0.154510, 0.139696,
0.310846, 0.262009, -0.029205, -0.102432, 0.307433, 0.354391, 0.031638,
-0.024673, -0.300724, -0.223181, -0.032146, 0.083906, 0.206649, 0.130043,
0.226294, 0.028078, -0.043207, 0.313385, 0.234959, 0.029876, -0.011824,
0.067378, 0.154421, 0.109094, -0.426046, -0.567933, -0.131117, 0.032571,
-0.042579, -0.133004, 0.104716, 0.204647, -0.018671, 0.111897, -0.077778,
0.068694, -0.171284, -0.202598, -0.049498, 0.131387, 0.232945, 0.090187,
0.137378, -0.150930, -0.220908, -0.235758, -0.072389, 0.065547, -0.175764,
0.007007, 0.052182, -0.037747, -0.100959, -0.396486, -0.196395, 0.241168,
-0.229930, 0.068248, -0.028620, 0.113565, -0.203604, 0.022065, 0.149447,
-0.040511, 0.009266, 0.078089, 0.119100, -0.149713, -0.108503, 0.088283,
0.101872, -0.014418, -0.220066, -0.455388, -0.089243, 0.066408, -0.163943,
-0.271931, 0.155011, 0.176885, -0.109034, 0.010049, -0.060000, -0.090051,
0.362748, 0.084588, -0.093191, -0.257420, 0.234382, -0.204185, -0.185660,
0.038978, -0.180305, -0.201932, 0.058174, 0.204704, 0.030806, -0.295941,
-0.142774, 0.240122, -0.000484, 0.141188, 0.109060, -0.032433, 0.096146,
0.326995, 0.127280, 0.232800, -0.496613, -0.135647, -0.068279, 0.127225,
-0.140732, -0.377970, -0.139864, -0.062894, 0.361094, 0.254778, -0.024366,
0.391247, -0.382722, -0.255403, -0.044358, -0.224925, 0.393691, 0.325094,
0.133854, -0.147898, 0.259567, 0.256653, 0.135596, 0.181709, -0.432095,
-0.530017, -0.230045, 0.004459, 0.378325, 0.476000, 0.059787, -0.185005,
0.191192, 0.190836, -0.077534, 0.068355, 0.097122, 0.124060, -0.017733,
0.218551, 0.086810, 0.202118, 0.015103, -0.156957, 0.042844, 0.001126,
0.147901, 0.116530, -0.178001, 0.225755, 0.041218, -0.061581, -0.307006,
-0.236132, 0.064867, -0.037160, 0.218221, -0.031101, -0.157354, 0.015034,
0.251406, 0.301926, 0.063385, -0.022444, 0.113022, 0.407806, -0.182217,
-0.226254, 0.039117, -0.137818, -0.100196, 0.020947, -0.054223, -0.122667,
-0.086660, -0.100510, 0.421007, 0.053103, -0.002624, 0.083775, -0.106500,
-0.331288, 0.104456, -0.139112, 0.259176, -0.069737, -0.103264, 0.172798,
-0.111568, -0.210600, 0.108634, 0.036263, -0.027275, -0.137678, 0.064683,
-0.100904, 0.100506, 0.062598, -0.000835, -0.203732, 0.283563, 0.103232,
0.187697, -0.319106, 0.162042, 0.183838, -0.056631, 0.413967, -0.418357,
-0.343289, 0.013876, -0.139684, 0.002637, -0.297735, -0.114454, -0.222143,
0.366091, 0.247279, 0.066031, 0.110445, 0.152449, -0.003219, 0.144652,
0.117414, -0.340368, -0.401681, -0.031997, 0.158840, -0.130777, -0.152629,
0.138936, 0.051418, -0.430334, -0.277468, 0.048016, 0.015008, -0.017447,
-0.252139, 0.161938, 0.086002, -0.447711, -0.414281, -0.053290, -0.177355,
0.286748, 0.211279, -0.064344, 0.090327, -0.183199, -0.148761, 0.122758,
0.026259, -0.317191, -0.348092, 0.071814, 0.222071, -0.383688, -0.075726,
0.068990, -0.255218, 0.216378, -0.057396, -0.196168, 0.075094, -0.176948,
0.142203, 0.012248, -0.087945, 0.337111, 0.106033, 0.009176, 0.326683,
-0.396150, -0.404846, -0.030197, 0.240264, -0.200145, -0.456037, 0.143875,
0.062423, 0.040650, 0.248670, -0.031491, 0.217425, -0.286637, -0.309789,
-0.136379, 0.217471, -0.216144, 0.051728, 0.112689, 0.093881, -0.217240,
-0.376106};

const float bias_matrix_0[100] = {
0.009914, -0.018938, 0.080286, 0.004511, 0.057654, -0.073333, -0.001467,
-0.042760, -0.000169, 0.179795, -0.008484, -0.001266, 0.150289, 0.015080,
0.046183, 0.003015, 0.012974, 0.176120, 0.000071, 0.002996, -0.027919,
0.018945, 0.004733, 0.048165, 0.024014, -0.108440, 0.123946, -0.065178,
-0.063864, 0.009843, 0.285505, 0.007079, -0.028878, 0.024339, -0.031325,
0.057982, -0.005832, 0.024761, 0.066177, 0.013322, 0.021750, -0.007595,
0.197690, 0.086092, -0.015479, -0.093012, 0.039737, 0.062490, 0.026058,
-0.007470, -0.173942, 0.005542, 0.168801, -0.013452, 0.204918, -0.219750,
-0.018635, 0.174996, -0.134659, -0.058744, -0.005793, -0.044361, 0.004884,
-0.020843, 0.074715, -0.037754, -0.002296, -0.220828, 0.044981, -0.000500,
-0.026148, 0.013843, -0.057689, -0.006085, -0.002065, -0.003584, -0.010231,
-0.014161, 0.113955, -0.000275, -0.040207, -0.016535, -0.013478, 0.008092,
0.174396, 0.007283, 0.265789, 0.010441, 0.065584, 0.263444, 0.008090,
0.014983, -0.003904, -0.163382, 0.278078, 0.034901, -0.003260, 0.080623,
0.002833, -0.017203};

const float weight_matrix_1[300] = {
-0.181887, 0.052914, 0.275811, -0.278901, 0.350644, -0.218993, 0.135012,
-0.274278, -0.195111, 0.364821, 0.101142, 0.252637, 0.288870, 0.320265,
-0.001890, -0.203584, -0.352075, 0.261725, -0.142389, 0.051475, -0.116958,
0.276678, -0.070176, 0.260320, -0.019296, -0.292533, 0.190559, 0.095076,
-0.180465, -0.134096, 0.505396, 0.218196, 0.194824, 0.286503, -0.208167,
0.098411, -0.046960, 0.183595, 0.164301, -0.097538, -0.024963, -0.194989,
0.336393, 0.156426, 0.090701, -0.193444, -0.345287, 0.355739, 0.114694,
0.203425, -0.264337, 0.309564, 0.250522, -0.193730, 0.244963, -0.083735,
-0.032881, 0.101443, -0.177059, 0.062714, 0.143945, -0.023434, -0.104861,
0.114801, 0.300161, 0.084052, -0.320461, -0.143211, -0.113450, 0.147807,
-0.332575, -0.078063, -0.008840, 0.258740, 0.201226, -0.140322, -0.099690,
-0.107732, 0.182959, 0.176410, -0.149279, 0.057785, 0.099504, -0.050846,
0.422931, -0.036686, 0.435209, -0.359774, 0.208209, 0.347633, 0.315450,
-0.119754, -0.038044, -0.214778, 0.425281, 0.306318, -0.124603, 0.133568,
0.030256, 0.071708, 0.142392, -0.083393, -0.248429, 0.173198, 0.018389,
-0.219010, 0.251918, 0.055829, -0.157965, -0.080379, 0.259219, -0.216448,
0.080825, -0.239737, 0.169655, -0.022671, 0.123181, 0.145485, -0.088639,
-0.149621, -0.156107, -0.056682, 0.186255, 0.239701, 0.229430, 0.113534,
-0.185792, 0.087529, -0.137092, -0.050959, 0.030360, 0.219039, 0.116825,
-0.026630, -0.022409, 0.112742, -0.207780, 0.111647, 0.179894, 0.121570,
0.160613, -0.135855, 0.215181, -0.131263, -0.145271, 0.056495, 0.233512,
-0.235791, 0.187920, 0.141643, -0.207664, 0.187202, 0.089794, 0.090079,
0.015517, 0.114576, 0.258881, -0.268250, -0.071633, 0.097585, -0.189439,
-0.048674, 0.124070, -0.237425, 0.004195, -0.072103, -0.058224, -0.037067,
0.280245, 0.051531, -0.023916, -0.087080, -0.144378, -0.180130, -0.115356,
0.069325, 0.035827, 0.207071, -0.165036, 0.188195, -0.044158, 0.106200,
-0.156391, 0.105901, -0.052990, 0.168574, -0.019178, -0.014866, 0.114319,
0.164208, 0.170350, 0.219143, 0.101963, 0.016513, -0.095948, -0.036336,
0.110120, -0.234234, -0.262423, 0.041211, -0.014736, 0.014805, -0.248741,
-0.178834, -0.234685, 0.095899, -0.239647, 0.182628, -0.184798, -0.214025,
0.030006, -0.033583, -0.364576, -0.124482, -0.249378, -0.006678, 0.002945,
-0.160699, 0.245291, 0.007456, -0.062130, -0.027180, -0.210453, -0.224539,
-0.059592, 0.168739, -0.320169, 0.195612, -0.006100, -0.118411, -0.208727,
-0.173088, 0.248591, -0.021582, 0.066453, -0.287172, -0.205109, -0.134980,
-0.257689, -0.285205, -0.083856, -0.206222, -0.061686, -0.254942, 0.101528,
0.270387, -0.175334, -0.059126, -0.194753, 0.203491, -0.007992, 0.114337,
-0.288076, 0.194164, -0.276102, 0.241232, 0.054884, -0.426453, -0.000295,
0.299317, 0.085060, 0.025207, -0.041166, 0.093948, -0.020367, 0.292158,
0.317991, 0.251186, -0.258208, -0.178230, 0.014453, -0.235547, 0.282277,
0.058782, 0.204075, 0.140532, 0.142184, 0.059459, -0.214128, -0.121498,
0.287374, 0.158953, -0.267902, -0.273795, -0.305555, -0.074796, -0.306040,
0.200026, -0.141346, -0.237479, -0.084770, 0.067620, -0.199388, 0.143197,
-0.475666, -0.162803, 0.091862, -0.326081, -0.102332, -0.027314};

const float bias_matrix_1[3] = {-0.019752, 0.001997, 0.013343};

// Pointer to weight matrices
const float* weights[NUM_LAYERS - 1] = {weight_matrix_0, weight_matrix_1};
const float* biases[NUM_LAYERS - 1] = {bias_matrix_0, bias_matrix_1};

// Vector of types of activation
int activation[NUM_LAYERS - 1] = {2, 3};

// Vectors for intermediate calculations and results
float input[4] = {0, 0};
float hidden_output_0[100] = {0};
float prediction[3] = {0};

// Array of pointers to arrays that are used for calculations
float* help_arrays[NUM_LAYERS] = {input, hidden_output_0, prediction};


// A function that multiplies a matrix and a vector
void mat_vec_mul(const float* matrix, float* vector, float* result,
                 int mat_height, int mat_width)
{
    // Iterate over rows of matrix, to calculate elements of resulting vector
    for(int i = 0; i < mat_height; i++)
    {
        // Clear result vector, just to make sure
        result[i] = 0;

        // Iterate over columns of matrix
        for(int j = 0; j < mat_width; j++)
        {
            // Calculate dot product
            result[i] += matrix[i*mat_width + j]*vector[j];
        }
    }
}

// Calculates sigmoid of vector, in place
void sigmoid(float* vector, int vec_len)
{
    // Iterate over vector elements and calculate sigmoid
    for(int i = 0; i < vec_len; i++)
    {
        // Calculate elementwise sigmoid
        vector[i] = 1/(1 + exp(-vector[i]));
    }
}

// Calculated softmax of vector, in plase
void softmax(float* vector, int vec_len)
{
    // Variable for holding sum of exp of vector
    float sum_of_exp = 0;

    // Calculate exp of vector, and sum of exp of vector
    for(int i = 0; i < vec_len; i++)
    {
        vector[i] = exp(vector[i]);
        sum_of_exp += vector[i];
    }

    // Calculate softmax
    for(int i = 0; i < vec_len; i++)
    {
        vector[i] = vector[i]/sum_of_exp;
    }
}

// Predict based on content of input variable
void predict()
{
    // Iterate over number of layers in network
    for(int i = 0; i < NUM_LAYERS - 1; i++)
    {
        // Multiply output of previous layer with weights of current layer
        mat_vec_mul(weights[i], help_arrays[i], help_arrays[i + 1],
                    num_neurons[i + 1], num_neurons[i]);

        // Add biases
        for(int j = 0; j < num_neurons[i + 1]; j++)
        {
            help_arrays[i + 1][j] += biases[i][j];
        }

        // Call activation function
        switch(activation[i])
        {
            case LINEAR_ACTIVATION:
                // Vector stays the same
                break;

            case SIGMOID_ACTIVATION:
                // Call sigmoid activation
                sigmoid(help_arrays[i + 1], num_neurons[i + 1]);
                break;

            case SOFTMAX_ACTIVATION:
                // Call softmax activation
                softmax(help_arrays[i + 1], num_neurons[i + 1]);
                break;
        }
    }
}
